---
title: "Predicting the Quality of Barbell Lifting Exercise"
author: "Zhen Wang"
date: "January 29, 2016"
output: html_document
---

## My GitHub Link for this project: https://github.com/zweinstein/PredictWeightLiftingQuality

## Synopsis

Now people regularly quantify how much physical activity they do using devices such as Jawbone Up and Fitbit. It is interesting to also use such devices to quantify how well people do a particular activity. Here I built a machine learning algorithm to predict the quality of barbell lifting from activity monitors. The model was built on data collected from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants while they performed barbell lifting exercise in five different fashions. More information about the experimental design and data collection is available on the [Human Activity Recognition website][1] and the [paper by Velloso, E. et al, 2013][2].

## Set global and markdown global options

```{r}
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    fig.width   = 10,       # the width for plots created by code chunk
    fig.height  = 10,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    message     = FALSE,     # if FALSE knitr will not display any messages generated by code
    strip.white = FALSE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document

save.scipen <- getOption("scipen") #save existing scipen option
options(scipen = 10)        # set new scipen - numbers with less than 10 digits will be displayed fixed.
```

## Data Processing

### Raw Data Source and Loading into R

The [training data][3] and [test data][4] were downloaded from the [Human Activity Recognition website][1] and loaded into R:

```{r}
if(!(file.exists("pml-training.csv"))) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, "pml-training.csv", method="curl")
}
if(!(file.exists("pml-testing.csv"))) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileUrl, "pml-testing.csv", method="curl")
}

pml_training <- read.csv("pml-training.csv") 
pml_testing <- read.csv("pml-testing.csv")
```

### Data Cleaning and Rationale for Selecting Predictors and Prediction Algorithms

There are 5 possible outcomes of the prediction (the "classe" variable in the training dataset): (A) exactly according to the specification; (B) throwing the elbows to the front; (C) lifting the dumbbell only halfway; (D) lowering the dumbbell only halfway; (E) throwing the hips to the front. Class A corresponds to the correct execution of the exercise, while the other 4 classes correspond to common mistakes. The goal of the prediction algorithm is to identify whether the dumbbell lifting was performed correctly (Class A) or identify the mistake (Classes B to E).

The accuracy of the prediction algorithm will be evaluated on each row of the pml-testing dataset, irrelevant of the username, timestamps and the "window" features. Therefore, I removed the first 7 columns in both datasets since those variables will not be used as predictors.

```{r}
pml_training <- pml_training[, -(1:7)]
pml_testing <- pml_testing[, -(1:7)]
```

There are columns in both datasets that contain many missing values (NAs), which also need to be removed since they cannot be used as predictors. I used the whatis() function in "YaleToolkit" package to calculate the number of NAs in each column of the dataframes.

```{r}
library(YaleToolkit)
# only keep columns that contain NAs < 10% of total rows
# i.e., < 1962 for the training set; < 2 for the testing set
lowNAs <- (whatis(pml_training)$missing < 1962 & whatis(pml_testing)$missing < 2)
pml_training <- pml_training[, lowNAs]
pml_testing <- pml_testing[, lowNAs]
```

Now there are 53 variables remaining in both datasets (52 potential predictors), which are significantly fewer than the original datasets (160 columns) but still a lot. There are 19622 observations in the pml-training dataset, and the scales of 52 potential predictors are very different. **Due to these features of the data, I decided to use random forests as my prediction algorithm because:**

- it can run efficiently on large datasets (e.g. 19622 observations and 53 variables in the pml-training dataset);
- it can handle many input variables even if the variables are correlated (e.g. 52 predictors);
- it can handle unscaled variables (e.g. 52 predictors on various scales), which reduces the need for additional data transformation steps that may cause overfitting.

(Note: More information and discussion on Random Forests [here][5].)

In addition, I have explored other prediction models such as "rpart", "gbm", "bayesglm" (not shown in this report for conciseness) and they did not perform as well as Random Forests in this particular project.

## Developing the Prediction Algorithms

### Data Splitting into Training and Testing Datasets

**I held out 20% of the pml-training data (not used in building the algorithms) as the final validation testing dataset in order to have an independent estimate of the out of the sample error rate:**

```{r}
library(caret)
set.seed(999)
# Split the original training data into two parts:
inBuild <- createDataPartition(y=pml_training$classe, p=0.8, list=FALSE)
# (1) training: 80% of pml_training data, used for building the model:
training <- pml_training[inBuild, ]
# (2) testing: 20% of pml_training data, held out to evaluate the model:
testing <- pml_training[-inBuild, ]
```

### Training with Random Forests

**I use the train() function in the caret package to build the prediction algorithms.** *The caret: train() function performs a series of training runs with a specified model by varying the parameters and/or folds within a resampling/cross-validation process, and optimizes the model based on the estimated out-of-sample errors for each run.* 

**To improve the computation efficiency, I loaded the 'doMC' library to allow parallel execution of the codes, and used k-fold cross-validation instead of the default bootstrapping resampling method.** The prediction algorithm was built on the training dataset (80% of the original pml-training data):

```{r}
kCV <- trainControl(method = "cv", number = 5) # 5-fold cross-validation

library(doMC)
registerDoMC(cores = 4) # allow parallel execution with 4 cores

library(e1071)
modrf <- train(classe ~., method="rf", trControl=kCV, data=training)
print(modrf)
modrf$finalModel 
```

### Validation and Estimating Out of Sample Error Rate

The prediction algorithm was validated on the testing dataset (20% of the original pml-training data, separate from the training data used to build the model):

```{r}
predrf <- predict(modrf, testing[, -53])
M <- confusionMatrix(predrf, testing[, 53])
M
```

**Based on the output of the confusionMatrix with the held-out testing dataset, I expect the out of sample error rate to be:** 

```{r}
acc_modrf <- as.numeric(M$overall[1]) # Accuracy
err_modrf <- 1 - acc_modrf # out of sample error estimate
err_modrf
```

Thus, when applying this model to predict the pml-testing data (20 samples in total), my overall successful predictions should be:

```{r}
acc_modrf*20 # expected successful predictions
```

Based on this estimate, I will most likely get all 20 questions correct on the quiz. Let's test it out:

## Application to the "pml-testing"" Dataset in the Project Quiz

```{r}
pml_testing$classe <- predict(modrf, pml_testing[, -53])
pml_testing$classe
```

**Quiz results: I got 20/20 correct, which confirms the out-of-sample error rate is indeed quite low (`r err_modrf`).**

## Analysis of Variable Importance and Future Directions

I used caret:varImp() function to calculate the variable importance for the developed prediction model, and plotted the top 20 (among all the 52) predictors to visualize the variable importance ranking: 

```{r}
Var.Imp <- varImp(modrf, scale = T) # the importance values are scaled to 0-100
plot(Var.Imp, main = "Variable Importance in the Predictoin Model")
```

The variable importance analysis is insightful for future experiment design and data analysis. For example, future experiment can focus on collecting and processing the data most critical for developing accurate prediction models such as the top 20 variables identified here:

```{r}
Var.Imp
```

## Limitation of the Model

The training and testing data were collected from 6 young healthy male participants aged between 20-28 years in a Unilateral Dumbbell Biceps Curl experiment. The prediction model may not perform well on data collected from other exercises or from different age groups.

## Computing Environment

This analysis was performed using the following processing environment:

```{r}
sessionInfo()
```

[1]:http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises
[2]:http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201
[3]:https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
[4]:https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
[5]:https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm